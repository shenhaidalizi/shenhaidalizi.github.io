---
layout:     post
title:      [强化学习]
subtitle:   [第六章笔记]
date:       [2023-03-17]
author:     Siyuan Zhou
header-img: img/post-bg-article.jpg
catalog: true
tags:
    - Artificial Intelligence
---

 强化学习：在于环境交互之中进行学习，在智能主体与环境的交互中，学习能最大化受益的行动模式。

![img](https://img-blog.csdnimg.cn/e0bb096c61a24e448de1281dbc6363df.png)

![点击并拖拽以移动](data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==)

智能主体：

- 按照某种策略，根据当前的状态选择合适的动作；
- 状态指的是智能主体对环境的一种解释；
- 动作反映了智能主体对环境主观能动的影响，动作带来的收益称为奖励；
- 只能主体可能知道也可能不知道环境变化的规律；

环境：

- 系统中智能主体以外的部分；
- 向智能主体反馈状态和奖励；
- 按照一定的规律发生变化；

强化学习特点：

![img](https://img-blog.csdnimg.cn/30451b99849d4f5087025f566731ec39.png)![点击并拖拽以移动](data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==)

-  基于评估：强化学习利用环境评估当前策略，以此为依据进行优化；
- 交互性：强化学习的数据在环境的交互中产生；
- 序列决策过程：智能主体在与环境的交互中需要作出一系列的决策，这些决策往往是前后关联的；

![img](https://img-blog.csdnimg.cn/bfa2b2cfc1c542a791cb0648724fd20c.png)

![点击并拖拽以移动](data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==)

离散马尔可夫过程：

- 一个随机过程实际上是一列随时间变化的随机变量，其中当时间是离散量时，一个随机过程剋有表示为{Xt}t = 0，1，2，...;
- 马尔科夫链：满足马尔可夫性的离散随机过程，也被称为离散马尔可夫过程； 

为了在序列决策中对目标进行优化，在马尔可夫随机过程框架中加入了奖励机制，奖励过程：

- 奖励函数R : S × S → R，其中R(St，St+1)描述了从第t步状态转义导第t + 1步状态所获得奖励；
- 在一个序列决策过程中，不同状态之间的转移产生了一系列的奖励；
- 引入奖励机制，这样可以衡量任意序列的优劣，即对序列决策进行评价；

在强化学习问题中，智能主体与环境交互过程中可自主决定所采取的动作，不同动作会对环境产生不同影响，引入动作：

- 定义智能主体能够采取的动作集合为A；
- 由于不同的动作对环境造成的影响不同，因此状态转移概率定义为Pr；
- 奖励可能受动作的影响，因此修改奖励函数；
- 动作集合A可以是有限的，也可以是无限的；
- 状态转移可是确定的，也可以是随机概率性的；
- 确定状态转移相当于发生从St到St+1的转移概率为1；

决策过程：

- 马尔可夫决策过程MDP = {S, A, Pr, R, γ}是刻画强化学习中环境的标准模式；
- 马尔可夫决策过程可用如下序列来表示：

![img](https://img-blog.csdnimg.cn/2e729546a62142b1a5d719a3df1e035e.png)

![点击并拖拽以移动](data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==)

轨迹长度可以是无限的，也可以有终止状态。有终止状态的问题叫做分段的，否则叫做持续的；分段问题中，一个从初始状态到终止状态的完整轨迹称为一个片段。

在机器人移动问题中：状态、行为、衰退系数、起始/终止状态、反馈、状态转移概率矩阵。

策略学习：一个好的策略是在当前状态下采取一个行动，该行动能够在未来收到最大化的反馈

- 价值函数：在第t步状态为s是，按照策略Π行动后未来所获得反馈值的期望；
- 动作-价值函数：在第t步状态为s时，按照策略Π采取动作a后，在未来所获得反馈值的期望；



# 策略优化与策略评估

## 强化学习的问题与求解

![img](https://img-blog.csdnimg.cn/831f645f5b3a497eb985a49795d55ff7.png)

![点击并拖拽以移动](data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==)

### 基于价值的求解方法

策略优化：

![img](https://img-blog.csdnimg.cn/bc27a6db09384b349159bfa53f81e0b4.png)![点击并拖拽以移动](data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==)

策略评估：

通过迭代计算贝尔曼方程进行评估：

- 动态规划；
- 蒙特卡洛采样；
- 时序差分

 动态规划的缺点：

- 智能主体需要事先知道状态转移概率；
- 无法处理状态集合大小无限的情况；

蒙特卡洛采样法的优点：

- 智能主体不必知道状态转移概率；
- 容易扩展到无限状态集合的问题中；

缺点：

- 状态集合比较大时，一个状态在轨迹可能非常稀疏，不利于估计期望；
- 在实际问题中，最终反馈需要在终止状态才能知晓，导致反馈周期较长；

# 强化学习求解

## 基于价值的求解方法

策略优化与策略评估结合：

![img](https://img-blog.csdnimg.cn/509a985cf46849a2b1ad24dd8c1f2512.png)

![点击并拖拽以移动](data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==)

Q学习中只有利用没有探索，所以收敛到非最优策略。

![img](https://img-blog.csdnimg.cn/9969dd137b83464897403a09ab71122f.png)![点击并拖拽以移动](data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==)

思路：

- 将q函数参数化，用一个非线性回归模型来拟合q函数；
- 用有限的参数刻画无限的状态；
- 由于回归函数的连续性，没有探索过的状态也可通过周围的状态来估计；



# 深度强化学习

![img](https://img-blog.csdnimg.cn/fdd08c39caff4bab90fab819655bb6bd.png)![点击并拖拽以移动](data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==)

- 损失函数刻画了q的估计值与当前值的平方误差；
- 利用梯度下降法优化参数θ；
- 如果用深度神经网络来拟合q函数，则算法称为深度Q学习；

深度Q学习的两个不稳定因素;

1. 相邻的样本来自同一条轨迹，样本之间相关性太强，集中优化相关性强的样本可能导致神经网络在其他样本上效果下降；
2. 在损失函数中，q函数的值既用来估计目标值，又用来计算当前值。现在这两处的q函数通过θ有所关联，可能导致优化时不稳定；

![img](https://img-blog.csdnimg.cn/b4dca623cfad409abd22d2091849fb11.png)![点击并拖拽以移动](data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==)

![img](https://img-blog.csdnimg.cn/e4cc9ad8148f4875ba53218243876520.png)

![点击并拖拽以移动](data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==)

经验重现：

- 相邻的样本来自同一条轨迹，样本之间相关性太强，集中优化相关性强的样本可能导致神经网络在其他样本上效果下降。
- 将过去的经验存储下来，每次将新的样本加入到存储中去，并从存储中采样一批样本进行优化：解决了样本相关性强的问题，重用经验，提高了信息利用的效率。

目标网络在损失函数中，q函数的值既用来估计目标值，又用来计算当前值。两处q通过参数θ关联，可能导致优化时不稳定。

损失函数的两个q函数用不同的参数计算：

- 用于计算估计值的q使用参数θ-计算，这个网络叫做目标网络；
- 用于计算当前值的q使用参数θ计算；
- 保持θ-的值相对稳定，更新多次后才同步两者的值；
